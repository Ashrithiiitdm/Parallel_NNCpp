        -:    0:Source:main.c
        -:    0:Graph:main.gcno
        -:    0:Data:main.gcda
        -:    0:Runs:2
        -:    1:#include <stdio.h>
        -:    2:#include <stdlib.h>
        -:    3:#include <math.h>
        -:    4:#include <stdbool.h>
        -:    5:#include <time.h>
        -:    6:
        -:    7:// Function to generate random numbers between 0 and 1
   159020:    8:double get_random(){
   159020:    9:    return (rand() / (double)RAND_MAX) * 2 - 1;
        -:   10:}
        -:   11:
        -:   12:// Define the Neuron struct
        -:   13:typedef struct{
        -:   14:    double *weights;
        -:   15:    double *wgrad;
        -:   16:    double bias;
        -:   17:    double bgrad;
        -:   18:    int input_size;
        -:   19:}Neuron;
        -:   20:
        -:   21:// Function to initialize a Neuron
      220:   22:void init_neuron(Neuron *n, int input_size){
        -:   23:
      220:   24:    n->input_size = input_size;
      220:   25:    n->weights = (double *)calloc(input_size, sizeof(double));
      220:   26:    n->wgrad = (double *)calloc(input_size, sizeof(double));
      220:   27:    n->bias = 0.01 * get_random();
      220:   28:    n->bgrad = 0.0;
        -:   29:
   159020:   30:    for(int i = 0; i < input_size; i++){
   158800:   31:        n->weights[i] = get_random();
        -:   32:    }
      220:   33:}
        -:   34:
        -:   35:// Free memory
    #####:   36:void free_neuron(Neuron *n){
    #####:   37:    free(n->weights);
    #####:   38:    free(n->wgrad);
    #####:   39:}
        -:   40:
        -:   41:// Function to reset gradients of the Neuron
132000000:   42:void zero_grad(Neuron *n){
        -:   43:
95412000000:   44:    for(int i = 0; i < n->input_size; i++){
95280000000:   45:        n->wgrad[i] = 0.0;
        -:   46:    }
132000000:   47:    n->bgrad = 0.0;
132000000:   48:}
        -:   49:
        -:   50:// Function to calculate feed-forward output
134200000:   51:double feed_forward(Neuron *n, double *inputs){
        -:   52:
134200000:   53:    double sum = n->bias;
97002200000:   54:    for(int i = 0; i < n->input_size; i++){
96868000000:   55:        sum += inputs[i] * n->weights[i];
        -:   56:    }
134200000:   57:    return sum;
        -:   58:}
        -:   59:
        -:   60:// Function to update gradients during backpropagation
132000000:   61:void backpropagation(Neuron *n, double *last_input, double grad){
        -:   62:
132000000:   63:    n->bgrad += grad;
95412000000:   64:    for(int i = 0; i < n->input_size; i++){
95280000000:   65:        n->wgrad[i] += grad * last_input[i];
        -:   66:    }
132000000:   67:}
        -:   68:
        -:   69:// Function to update weights and bias
132000000:   70:void descend(Neuron *n, double learning_rate){
        -:   71:
132000000:   72:    n->bias -= learning_rate * n->bgrad;
95412000000:   73:    for(int i = 0; i < n->input_size; i++){
95280000000:   74:        n->weights[i] -= learning_rate * n->wgrad[i];
        -:   75:    }
132000000:   76:}
        -:   77:
        -:   78:// Define the Layer struct
        -:   79:typedef struct{
        -:   80:    Neuron *neurons;    // Array of Neurons
        -:   81:    double *last_input; // Array for storing the last input
        -:   82:    int input_size;     // Number of inputs to the layer
        -:   83:    int output_size;    // Number of neurons in the layer
        -:   84:}Layer;
        -:   85:
        -:   86:// Function to initialize a Layer
        4:   87:void init_layer(Layer *l, int input_size, int output_size){
        -:   88:
        4:   89:    l->input_size = input_size;
        4:   90:    l->output_size = output_size;
        -:   91:
        4:   92:    l->neurons = (Neuron *)malloc(output_size * sizeof(Neuron));   // Allocate memory for neurons
        4:   93:    l->last_input = (double *)malloc(input_size * sizeof(double)); // Allocate memory for last input
        -:   94:
        -:   95:    // Initialize each neuron in the layer
      224:   96:    for(int i = 0; i < output_size; i++){
      220:   97:        init_neuron(&l->neurons[i], input_size); 
        -:   98:    }
        4:   99:}
        -:  100:
        -:  101:// free memory
    #####:  102:void free_layer(Layer *l){
        -:  103:
    #####:  104:    free(l->neurons);    
    #####:  105:    free(l->last_input); 
    #####:  106:}
        -:  107:
        -:  108:// Function to reset gradients of all neurons in the layer
  2400000:  109:void zero_grad_layer(Layer *l){
        -:  110:
134400000:  111:    for(int i = 0; i < l->output_size; i++){
132000000:  112:        zero_grad(&l->neurons[i]);
        -:  113:    }
  2400000:  114:}
        -:  115:
        -:  116:// Function to calculate feed-forward output for the entire layer
  2440000:  117:double *feed_forward_layer(Layer *l, double *inputs){
        -:  118:
  2440000:  119:    l->last_input = inputs;                                              // Store the last input
  2440000:  120:    double *outputs = (double *)malloc(l->output_size * sizeof(double)); // Allocate memory for outputs
        -:  121:
        -:  122:    // Calculate the output for each neuron
136640000:  123:    for(int i = 0; i < l->output_size; i++){
134200000:  124:        outputs[i] = feed_forward(&l->neurons[i], inputs); // Call the feed_forward function from Neuron.h
        -:  125:    }
        -:  126:
  2440000:  127:    return outputs;
        -:  128:}
        -:  129:
        -:  130:// Function to perform backpropagation on the layer
  2400000:  131:void backward(Layer *l, double *grad){
        -:  132:
134400000:  133:    for(int i = 0; i < l->output_size; i++){
132000000:  134:        backpropagation(&l->neurons[i], l->last_input, grad[i]); // Call backpropagation from Neuron.h
        -:  135:    }
  2400000:  136:}
        -:  137:
        -:  138:// Function to perform gradient descent on all neurons in the layer
  2400000:  139:void descend_layer(Layer *l, double learning_rate){
        -:  140:
134400000:  141:    for(int i = 0; i < l->output_size; i++){
132000000:  142:        descend(&l->neurons[i], learning_rate); // Call descend function from Neuron.h
        -:  143:    }
  2400000:  144:}
        -:  145:
        -:  146:// Define the MSE structure
        -:  147:typedef struct{
        -:  148:    double *last_input;  // Array to store last input
        -:  149:    double *last_target; // Array to store last target
        -:  150:    double *grad;        // Array to store gradient
        -:  151:    int size;            // Size of the input and target arrays
        -:  152:}MSE;
        -:  153:
        -:  154:// Function to initialize an MSE object
    #####:  155:void init_mse(MSE *mse){
        -:  156:
    #####:  157:    mse->last_input = NULL;
    #####:  158:    mse->last_target = NULL;
    #####:  159:    mse->grad = NULL;
    #####:  160:    mse->size = 0;
    #####:  161:}
        -:  162:
        -:  163:// Function to free memory for an MSE object
    #####:  164:void free_mse(MSE *mse){
        -:  165:
    #####:  166:    free(mse->last_input);
    #####:  167:    free(mse->last_target);
    #####:  168:    free(mse->grad);
    #####:  169:}
        -:  170:
        -:  171:// Function to calculate the MSE loss
  1200000:  172:double feed_forward_mse(MSE *mse, double *inputs, double *targets, int size)
        -:  173:{
  1200000:  174:    mse->last_input = (double *)malloc(size * sizeof(double));
  1200000:  175:    mse->last_target = (double *)malloc(size * sizeof(double));
  1200000:  176:    mse->size = size;
        -:  177:
        -:  178:    // Copy inputs and targets to last_input and last_target
 13200000:  179:    for(int i = 0; i < size; i++){
        -:  180:
 12000000:  181:        mse->last_input[i] = inputs[i];
 12000000:  182:        mse->last_target[i] = targets[i];
        -:  183:    }
        -:  184:
        -:  185:    // Compute the MSE
  1200000:  186:    double sum = 0;
 13200000:  187:    for(int i = 0; i < size; i++){
 12000000:  188:        double s = mse->last_input[i] - mse->last_target[i];
 12000000:  189:        sum += s * s;
        -:  190:    }
        -:  191:
  1200000:  192:    return sum / size;
        -:  193:}
        -:  194:
        -:  195:// Function to calculate the gradient
  1200000:  196:void backward_mse(MSE *mse, double grad){
        -:  197:
  1200000:  198:    mse->grad = (double *)malloc(mse->size * sizeof(double));
        -:  199:
 13200000:  200:    for(int i = 0; i < mse->size; i++){
 12000000:  201:        mse->grad[i] = 2 * (mse->last_input[i] - mse->last_target[i]) / mse->size;
 12000000:  202:        mse->grad[i] *= grad;
        -:  203:    }
  1200000:  204:}
        -:  205:
        -:  206:// Define the Sigmoid structure
        -:  207:typedef struct{
        -:  208:
        -:  209:    double *last_input;  // Array to store last input
        -:  210:    double *grad;        // Array to store gradient
        -:  211:    double *last_output; // Array to store last output
        -:  212:    int size;            // Size of the input/output arrays
        -:  213:}Sigmoid;
        -:  214:
        -:  215:// Function to initialize a Sigmoid object
        4:  216:void init_sigmoid(Sigmoid *s){
        -:  217:
        4:  218:    s->last_input = NULL;
        4:  219:    s->grad = NULL;
        4:  220:    s->last_output = NULL;
        4:  221:    s->size = 0;
        4:  222:}
        -:  223:
        -:  224:// Function to free memory for a Sigmoid object
    #####:  225:void free_sigmoid(Sigmoid *s){
        -:  226:
    #####:  227:    free(s->last_input);
    #####:  228:    free(s->grad);
    #####:  229:    free(s->last_output);
    #####:  230:}
        -:  231:
        -:  232:// Function to calculate feed-forward output for Sigmoid
  2440000:  233:double *feed_forward_sigmoid(Sigmoid *s, double *inputs, int size){
        -:  234:
  2440000:  235:    s->last_input = (double *)malloc(size * sizeof(double));
  2440000:  236:    s->last_output = (double *)malloc(size * sizeof(double));
  2440000:  237:    s->size = size;
        -:  238:
        -:  239:    // Copy inputs to last_input
136640000:  240:    for(int i = 0; i < size; i++){
134200000:  241:        s->last_input[i] = inputs[i];
        -:  242:    }
        -:  243:
        -:  244:    // Compute the Sigmoid activation
  2440000:  245:    double *outputs = s->last_output;
136640000:  246:    for(int i = 0; i < size; i++){
134200000:  247:        outputs[i] = 1 / (1 + exp(-inputs[i]));
        -:  248:    }
        -:  249:
  2440000:  250:    return outputs;
        -:  251:}
        -:  252:
        -:  253:// Function for backward propagation (using chain gradients)
  1200000:  254:void backward_chain(Sigmoid *s, double *chain_grad, int size){
        -:  255:
  1200000:  256:    s->grad = (double *)malloc(size * sizeof(double));
        -:  257:
 13200000:  258:    for(int i = 0; i < size; i++){
 12000000:  259:        s->grad[i] = s->last_output[i] * (1 - s->last_output[i]) * chain_grad[i];
        -:  260:    }
  1200000:  261:}
        -:  262:
        -:  263:// Function for backward propagation using the previous layer's gradients
  1200000:  264:void backward_layer(Sigmoid *s, Layer *prev_layer){
        -:  265:
  1200000:  266:    s->grad = (double *)malloc(s->size * sizeof(double));
        -:  267:
121200000:  268:    for(int i = 0; i < s->size; i++){
        -:  269:
120000000:  270:        double sum = 0;
        -:  271:
1320000000:  272:        for(int j = 0; j < prev_layer->output_size; j++){
1200000000:  273:            sum += prev_layer->neurons[j].weights[i] * prev_layer->neurons[j].wgrad[i];
        -:  274:        }
        -:  275:
120000000:  276:        s->grad[i] = s->last_output[i] * (1 - s->last_output[i]) * sum;
        -:  277:    }
  1200000:  278:}
        -:  279:
       24:  280:void reverse_bytes(char *bytes, int size){
        -:  281:
       72:  282:    for (int i = 0; i < size / 2; i++){
        -:  283:
       48:  284:        char temp = bytes[i];
       48:  285:        bytes[i] = bytes[size - i - 1];
       48:  286:        bytes[size - i - 1] = temp;
        -:  287:    }
       24:  288:}
        -:  289:
        2:  290:bool load_data(double ***train_images, int **train_labels, double ***test_images, int **test_labels, int *train_size, int *test_size){
        -:  291:
        -:  292:    // Load training labels
        2:  293:    FILE *file_labels = fopen("dataset/train-labels.idx1-ubyte", "rb");
        2:  294:    if(!file_labels){
    #####:  295:        return false;
        -:  296:    }
        -:  297:
        -:  298:    int magic_number, no_of_items;
        2:  299:    fread(&magic_number, sizeof(magic_number), 1, file_labels);
        2:  300:    reverse_bytes((char *)&magic_number, sizeof(magic_number));
        2:  301:    fread(&no_of_items, sizeof(no_of_items), 1, file_labels);
        2:  302:    reverse_bytes((char *)&no_of_items, sizeof(no_of_items));
        -:  303:
        2:  304:    *train_labels = (int *)malloc(no_of_items * sizeof(int));
   120002:  305:    for(int i = 0; i < no_of_items; i++){
        -:  306:        char label;
   120000:  307:        fread(&label, sizeof(label), 1, file_labels);
   120000:  308:        (*train_labels)[i] = (int)label;
        -:  309:    }
        2:  310:    fclose(file_labels);
        -:  311:
        -:  312:    // Load training images
        2:  313:    FILE *images = fopen("dataset/train-images.idx3-ubyte", "rb");
        2:  314:    if(!images){
    #####:  315:        return false;
        -:  316:    }
        -:  317:
        -:  318:    int magic_number_images, no_of_images, no_of_rows, no_of_columns;
        2:  319:    fread(&magic_number_images, sizeof(magic_number_images), 1, images);
        2:  320:    reverse_bytes((char *)&magic_number_images, sizeof(magic_number_images));
        2:  321:    fread(&no_of_images, sizeof(no_of_images), 1, images);
        2:  322:    reverse_bytes((char *)&no_of_images, sizeof(no_of_images));
        2:  323:    fread(&no_of_rows, sizeof(no_of_rows), 1, images);
        2:  324:    reverse_bytes((char *)&no_of_rows, sizeof(no_of_rows));
        2:  325:    fread(&no_of_columns, sizeof(no_of_columns), 1, images);
        2:  326:    reverse_bytes((char *)&no_of_columns, sizeof(no_of_columns));
        -:  327:
        2:  328:    *train_images = (double **)malloc(no_of_images * sizeof(double *));
   120002:  329:    for(int i = 0; i < no_of_images; i++){
        -:  330:        char image[784];
   120000:  331:        fread(image, sizeof(image), 1, images);
   120000:  332:        (*train_images)[i] = (double *)malloc(784 * sizeof(double));
 94200000:  333:        for (int j = 0; j < 784; j++){
 94080000:  334:            (*train_images)[i][j] = (double)((unsigned char)image[j]) / 255.0;
        -:  335:        }
        -:  336:    }
        2:  337:    fclose(images);
        2:  338:    *train_size = no_of_images;
        -:  339:
        -:  340:    // Load test labels
        2:  341:    FILE *test_labels_file = fopen("dataset/t10k-labels.idx1-ubyte", "rb");
        2:  342:    if(!test_labels_file){
    #####:  343:        return false;
        -:  344:    }
        -:  345:
        -:  346:    int test_magic_number, test_no_of_items;
        2:  347:    fread(&test_magic_number, sizeof(test_magic_number), 1, test_labels_file);
        2:  348:    reverse_bytes((char *)&test_magic_number, sizeof(test_magic_number));
        2:  349:    fread(&test_no_of_items, sizeof(test_no_of_items), 1, test_labels_file);
        2:  350:    reverse_bytes((char *)&test_no_of_items, sizeof(test_no_of_items));
        -:  351:
        2:  352:    *test_labels = (int *)malloc(test_no_of_items * sizeof(int));
    20002:  353:    for(int i = 0; i < test_no_of_items; i++){
        -:  354:        char label;
    20000:  355:        fread(&label, sizeof(label), 1, test_labels_file);
    20000:  356:        (*test_labels)[i] = (int)label;
        -:  357:    }
        2:  358:    fclose(test_labels_file);
        -:  359:
        -:  360:    // Load test images
        2:  361:    FILE *test_images_file = fopen("dataset/t10k-images.idx3-ubyte", "rb");
        2:  362:    if(!test_images_file){
    #####:  363:        return false;
        -:  364:    }
        -:  365:
        -:  366:    int test_magic_number_images, test_no_of_images, test_no_of_rows, test_no_of_columns;
        2:  367:    fread(&test_magic_number_images, sizeof(test_magic_number_images), 1, test_images_file);
        2:  368:    reverse_bytes((char *)&test_magic_number_images, sizeof(test_magic_number_images));
        2:  369:    fread(&test_no_of_images, sizeof(test_no_of_images), 1, test_images_file);
        2:  370:    reverse_bytes((char *)&test_no_of_images, sizeof(test_no_of_images));
        2:  371:    fread(&test_no_of_rows, sizeof(test_no_of_rows), 1, test_images_file);
        2:  372:    reverse_bytes((char *)&test_no_of_rows, sizeof(test_no_of_rows));
        2:  373:    fread(&test_no_of_columns, sizeof(test_no_of_columns), 1, test_images_file);
        2:  374:    reverse_bytes((char *)&test_no_of_columns, sizeof(test_no_of_columns));
        -:  375:
        2:  376:    *test_images = (double **)malloc(test_no_of_images * sizeof(double *));
    20002:  377:    for(int i = 0; i < test_no_of_images; i++){
        -:  378:        char image[784];
    20000:  379:        fread(image, sizeof(image), 1, test_images_file);
    20000:  380:        (*test_images)[i] = (double *)malloc(784 * sizeof(double));
        -:  381:
 15700000:  382:        for(int j = 0; j < 784; j++){
 15680000:  383:            (*test_images)[i][j] = (double)((unsigned char)image[j]) / 255.0;
        -:  384:        }
        -:  385:    }
        2:  386:    fclose(test_images_file);
        2:  387:    *test_size = test_no_of_images;
        -:  388:
        2:  389:    return true;
        -:  390:}
        -:  391:
       22:  392:double accuracy(int *predictions, int *labels, int size){
        -:  393:
       22:  394:    int correct = 0;
  1220022:  395:    for(int i = 0; i < size; i++){
  1220000:  396:        if(predictions[i] == labels[i]){
  1074725:  397:            correct++;
        -:  398:        }
        -:  399:    }
       22:  400:    return (double)correct / (double)size;
        -:  401:}
        -:  402:
        2:  403:int main(void){
        -:  404:
        2:  405:    srand(time(0));
        -:  406:
        -:  407:    double **train_images;
        -:  408:    int *train_labels;
        -:  409:    int train_size;
        -:  410:
        -:  411:    double **test_images;
        -:  412:    int *test_labels;
        -:  413:    int test_size;
        -:  414:
        2:  415:    bool flag = load_data(&train_images, &train_labels, &test_images, &test_labels, &train_size, &test_size);
        -:  416:
        2:  417:    if(!flag){
    #####:  418:        printf("Error loading data\n");
    #####:  419:        return 1;
        -:  420:    }
        -:  421:
        2:  422:    printf("Data loaded successfully\n");
        2:  423:    printf("Training images: %d\n", train_size);
        2:  424:    printf("Training labels: %d\n", train_size); // Same size as train_images
        2:  425:    printf("Test images: %d\n", test_size);
        2:  426:    printf("Test labels: %d\n", test_size);
        -:  427:
        -:  428:    Layer l1, l2;
        2:  429:    init_layer(&l1, 784, 100);
        2:  430:    init_layer(&l2, 100, 10);
        -:  431:    Sigmoid s1, s2;
        2:  432:    init_sigmoid(&s1);
        2:  433:    init_sigmoid(&s2);
        -:  434:
       22:  435:    for(int epoch = 0; epoch < 10; epoch++){
        -:  436:
       20:  437:        double learning_rate = 0.1;
       20:  438:        double mean_loss = 0.0;
        -:  439:
       20:  440:        int *predictions = (int *)malloc(train_size * sizeof(int));
        -:  441:
  1200020:  442:        for(int i = 0; i < train_size; i++){
        -:  443:
  1200000:  444:            int idx = i;
  1200000:  445:            double *image = train_images[idx];
  1200000:  446:            int label = train_labels[idx];
        -:  447:
  1200000:  448:            double *l1_output = feed_forward_layer(&l1, image);
  1200000:  449:            double *s1_output = feed_forward_sigmoid(&s1, l1_output, 100);
  1200000:  450:            double *l2_output = feed_forward_layer(&l2, s1_output);
  1200000:  451:            double *s2_output = feed_forward_sigmoid(&s2, l2_output, 10);
        -:  452:
  1200000:  453:            double target[10] = {0.0};
  1200000:  454:            target[label] = 1.0;
        -:  455:
  1200000:  456:            int prediction = 0;
 13200000:  457:            for(int j = 0; j < 10; j++){
        -:  458:
 12000000:  459:                if(s2_output[j] > s2_output[prediction]){
  2497472:  460:                    prediction = j;
        -:  461:                }
        -:  462:            }
        -:  463:
  1200000:  464:            predictions[i] = prediction;
        -:  465:
        -:  466:            MSE loss;
  1200000:  467:            double loss_value = feed_forward_mse(&loss, s2_output, target, 10);
        -:  468:
  1200000:  469:            mean_loss += loss_value;
  1200000:  470:            if(i % 500 == 0){
     2400:  471:                printf("Epoch: %d | Mean loss: %.4f\r", epoch + 1, mean_loss / (i + 1));
        -:  472:            }
        -:  473:
        -:  474:            // Backpropagation
  1200000:  475:            zero_grad_layer(&l1);
  1200000:  476:            zero_grad_layer(&l2);
        -:  477:
  1200000:  478:            backward_mse(&loss, 1.0);
        -:  479:
  1200000:  480:            backward_chain(&s2, loss.grad, 10);
  1200000:  481:            backward(&l2, s2.grad);
  1200000:  482:            backward_layer(&s1, &l2);
  1200000:  483:            backward(&l1, s1.grad);
        -:  484:
  1200000:  485:            descend_layer(&l1, learning_rate);
  1200000:  486:            descend_layer(&l2, learning_rate);
        -:  487:        }
        -:  488:
       20:  489:        double acc = accuracy(predictions, train_labels, train_size);
        -:  490:
       20:  491:        printf("Epoch: %d | Loss: %.4f | Training accuracy: %.2f%%\n", epoch + 1, mean_loss / train_size, acc * 100);
        -:  492:
       20:  493:        free(predictions);
        -:  494:    }
        -:  495:
        -:  496:    // Test the model
        2:  497:    int *test_predictions = (int *)malloc(test_size * sizeof(int));
        -:  498:
    20002:  499:    for(int i = 0; i < test_size; i++){
        -:  500:
    20000:  501:        double *image = test_images[i];
        -:  502:
    20000:  503:        double *l1_output = feed_forward_layer(&l1, image);
    20000:  504:        double *s1_output = feed_forward_sigmoid(&s1, l1_output, 100);
    20000:  505:        double *l2_output = feed_forward_layer(&l2, s1_output);
    20000:  506:        double *s2_output = feed_forward_sigmoid(&s2, l2_output, 10);
        -:  507:
    20000:  508:        int prediction = 0;
   220000:  509:        for(int j = 0; j < 10; j++){
        -:  510:
   200000:  511:            if(s2_output[j] > s2_output[prediction]){
    41500:  512:                prediction = j;
        -:  513:            }
        -:  514:        }
        -:  515:
    20000:  516:        test_predictions[i] = prediction;
        -:  517:    }
        -:  518:
        2:  519:    double test_acc = accuracy(test_predictions, test_labels, test_size);
        2:  520:    printf("Test accuracy: %.2f%%\n", test_acc * 100);
        -:  521:
        2:  522:    free(test_predictions);
        2:  523:    return 0;
        -:  524:}
